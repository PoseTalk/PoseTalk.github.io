<h1 align="center">ðŸ”¥PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for One-Shot Talking Head Generation<h1>



### [âœ¨Project Page](https://posetalk.github.io)  |  [ðŸ“•Paper](https://arxiv.org/abs)  |  [ðŸŽ‰Demo]()


<p align="center">
  <img src="./assets/images/teaser.jpg" alt="showcase">
</p>

TL,DR: We propose PoseTalk, an one-shot audio-driven talking head generation method which generate natural head poses controlled by text prompts and speaking audios. 

## Hightlights
<p align="center">
  <img src="./assets/images/method.jpg" alt="showcase">
  <br>Method overview. Videos can be watched on <a href="https://posetalk.github.io/"><strong>homepage</strong></a> ðŸ”¥
</p>

* We propose to generate poses from both text prompts and audio, where we use the text prompts to describe the long-term semantics of head motions and the audio to offer short-term variations and rhythm correspondence of the head movements. 
* Using the input audio and generated poses as input, we devise a refinement-based learning strategy to synthesize natural talking videos using two cascaded networks, i.e., CoarseNet, and RefineNet, yielding improved lip-synchronization performance. 



## Example Results
<p align="center">
  <img src="./assets/images/demo.gif" alt="showcase">
  <br>
  ðŸ”¥ Control the pose generation using different text prompts. Videos can be watched on <a href="https://posetalk.github.io/"><strong>homepage</strong></a> ðŸ”¥
</p>

<p align="center">
  <img src="./assets/images/EricPaul-audio-variation.gif" alt="showcase">
  <br>
  ðŸ”¥ Control the pose generation using different text prompts. Videos can be watched on <a href="https://posetalk.github.io/"><strong>homepage</strong></a> ðŸ”¥
</p>


